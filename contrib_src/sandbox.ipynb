{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how use the model api to run predictions and get information from the model. You can use it as basis to write your own experiements using the model in this container.\n",
    "\n",
    "To start, import _modelapi.model_, which will give us access to the api of the model in this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from modelapi import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 4 modalities\n",
      "Finished preprocessing\n",
      "Original shape: (240, 240, 155)\n",
      "Loaded shape: (240, 240, 155)\n",
      "Normalized images!\n",
      "Resizing did work!\n",
      "Converted images to tensors!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/contrib_src/evaluate.py:114: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  tensor_concat_var = torch.autograd.Variable(tensor_concat, volatile=True)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict('sample_data/sample.json')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the model for a list of sample data, then run a prediction on the first of the sample dataset, and then display the prediction result output.\n",
    "\n",
    "If the model does not provide any sample data, print an error message (this part also shows how to read meta info - like the model's name - from the models config)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.get_samples()\n",
    "if len(samples[\"files\"]) > 0:\n",
    "    inputFile = samples[\"folder\"] + \"/\" + samples[\"files\"][0]\n",
    "    result = model.predict(inputFile)\n",
    "    for i, output in enumerate(result[\"output\"]):\n",
    "        print(\"\\n\" + result[\"model\"][\"name\"] + \" Output \" + str(i) + \":\")\n",
    "        print(\"  Type:  \" + output[\"type\"])\n",
    "        print(\"  Name:  \" + output[\"name\"])\n",
    "        print(\"  Shape: \" + str(output[\"shape\"]))\n",
    "        print(\"  Prediction:\")\n",
    "        if output[\"type\"] in [\"mask_image\", \"image\"]:\n",
    "            # if output seems to be an image, try displaying it nicely with PIL\n",
    "            try:\n",
    "                import PIL\n",
    "                image = PIL.Image.fromarray(output[\"prediction\"])\n",
    "                display(image)\n",
    "            except:\n",
    "                display(output[\"prediction\"])\n",
    "        else:\n",
    "            display(output[\"prediction\"])\n",
    "else:\n",
    "    model_name = model.get_config()[\"meta\"][\"name\"]\n",
    "    print(model_name + \" does not provide any sample data, please try your own data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
